{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eeefb5fd0cc34349bf3afd459a3c5a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30091f80067749949f9f1a6cde80ff26",
              "IPY_MODEL_f5e109e7ef2646e3b3750693b1e84342",
              "IPY_MODEL_af95de76810444eb8f7ff15e3d424c8d"
            ],
            "layout": "IPY_MODEL_f8ce81f18d114ed78ff7948d8b3c1981"
          }
        },
        "30091f80067749949f9f1a6cde80ff26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40caa860441d47b99636693a5fc720e9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ea670a871bfe42d69aff5756b8dc4f38",
            "value": "Map:‚Äá100%"
          }
        },
        "f5e109e7ef2646e3b3750693b1e84342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48cdd74bb5c94ca6a3a5faed495d950f",
            "max": 182822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_788f193f3ba242de91c6d6b366db3346",
            "value": 182822
          }
        },
        "af95de76810444eb8f7ff15e3d424c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29913a3d3013414cafad797fe55d7db0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b49843b2579e48448539fc1afc06f4e2",
            "value": "‚Äá182822/182822‚Äá[00:31&lt;00:00,‚Äá6191.87‚Äáexamples/s]"
          }
        },
        "f8ce81f18d114ed78ff7948d8b3c1981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40caa860441d47b99636693a5fc720e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea670a871bfe42d69aff5756b8dc4f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48cdd74bb5c94ca6a3a5faed495d950f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "788f193f3ba242de91c6d6b366db3346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29913a3d3013414cafad797fe55d7db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b49843b2579e48448539fc1afc06f4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0d06d300e974949b82a7d4aa12d7342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c9ba36eee3a4e5e977a9abcbcc8143c",
              "IPY_MODEL_45b5b7e6829242f29217e70708fd1524",
              "IPY_MODEL_1cc15feae01c46c6877d95fe0fd52b66"
            ],
            "layout": "IPY_MODEL_eccac10aa2914da5bfe5202a92f72870"
          }
        },
        "9c9ba36eee3a4e5e977a9abcbcc8143c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d057242c3d48a3ba572d9919ab6786",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_08bc9611efbe4b0abdf2f99ffee4f8b2",
            "value": "Filter:‚Äá100%"
          }
        },
        "45b5b7e6829242f29217e70708fd1524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34cbf73abe6149c3b84c1b076be78481",
            "max": 182822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f7d6da4441d4fed8e5da7f1901408b4",
            "value": 182822
          }
        },
        "1cc15feae01c46c6877d95fe0fd52b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5053d2764c242ef960208688e56c545",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1596ba73b19a4cddb4598d2908f9fd6a",
            "value": "‚Äá182822/182822‚Äá[00:02&lt;00:00,‚Äá66732.70‚Äáexamples/s]"
          }
        },
        "eccac10aa2914da5bfe5202a92f72870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d057242c3d48a3ba572d9919ab6786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08bc9611efbe4b0abdf2f99ffee4f8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34cbf73abe6149c3b84c1b076be78481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f7d6da4441d4fed8e5da7f1901408b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5053d2764c242ef960208688e56c545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1596ba73b19a4cddb4598d2908f9fd6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "095424f7252c4b3db28c401539ca84b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad88c93bb95140b0986f8d20a11014b0",
              "IPY_MODEL_0037e9ed7b294866b1b08130e62c7b98",
              "IPY_MODEL_5deaeb3afb1d4174bccee22723bbe2e4"
            ],
            "layout": "IPY_MODEL_3c67d1a867814c6796f50349962ea715"
          }
        },
        "ad88c93bb95140b0986f8d20a11014b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d0c32c5892848a180264ebafd71af63",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bf6f7806b8ae4feb88672901b5ed6f5e",
            "value": "Map:‚Äá100%"
          }
        },
        "0037e9ed7b294866b1b08130e62c7b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c10b1e74c5949d0bf3b7f6e9f3b47b5",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74cc32161c954a71a853b4009f2c9f0a",
            "value": 500
          }
        },
        "5deaeb3afb1d4174bccee22723bbe2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_951214ee67bb412abb0d01fad9aad3c8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d6b5be51b20c422b9cda5a35d4ac21bb",
            "value": "‚Äá500/500‚Äá[00:00&lt;00:00,‚Äá1263.82‚Äáexamples/s]"
          }
        },
        "3c67d1a867814c6796f50349962ea715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0c32c5892848a180264ebafd71af63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6f7806b8ae4feb88672901b5ed6f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c10b1e74c5949d0bf3b7f6e9f3b47b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74cc32161c954a71a853b4009f2c9f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "951214ee67bb412abb0d01fad9aad3c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6b5be51b20c422b9cda5a35d4ac21bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v05_Y0pHrHcu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"‚úÖ Using device: CUDA (GPU NVIDIA)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö†Ô∏è Using device: CPU ‚Äî GPU not detected\")\n",
        "\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AENUf6OvHPV",
        "outputId": "bfc711db-2420-4a84-e39f-6ee4d5741f3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using device: CUDA (GPU NVIDIA)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n"
      ],
      "metadata": {
        "id": "-QHWOrkXwOEJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Use FP16 for memory efficiency\n",
        "    device_map={\"\": device},\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1SpqzcIsle5",
        "outputId": "9642264e-2c51-4bcc-bf42-0520cf0d11cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n‚öôÔ∏è Configuring LoRA...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                         # rank\n",
        "    lora_alpha=32,                # scaling\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFrFBm4qxD_m",
        "outputId": "32cda833-b500-4efb-829e-7359e5188216"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚öôÔ∏è Configuring LoRA...\n",
            "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(example):\n",
        "    question = example[\"question\"]\n",
        "\n",
        "    choices = {\n",
        "        \"A\": example[\"opa\"],\n",
        "        \"B\": example[\"opb\"],\n",
        "        \"C\": example[\"opc\"],\n",
        "        \"D\": example[\"opd\"]\n",
        "    }\n",
        "\n",
        "    # Convert numeric or string answer to letter\n",
        "    correct_raw = example[\"cop\"]\n",
        "    if isinstance(correct_raw, int):\n",
        "        index_to_letter = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
        "        correct = index_to_letter.get(correct_raw, None)\n",
        "    else:\n",
        "        correct = str(correct_raw).upper()\n",
        "\n",
        "    # Skip skipped or bad entries\n",
        "    if correct not in [\"A\", \"B\", \"C\", \"D\"]:\n",
        "        return {\"text\": None}\n",
        "\n",
        "    explanation = example[\"exp\"] if example[\"exp\"] else \"\"\n",
        "\n",
        "    # Build the user instruction\n",
        "    user_prompt = (\n",
        "        \"You are a medical assistant. \"\n",
        "        \"Answer the following multiple-choice medical question.\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"A: {choices['A']}\\n\"\n",
        "        f\"B: {choices['B']}\\n\"\n",
        "        f\"C: {choices['C']}\\n\"\n",
        "        f\"D: {choices['D']}\\n\\n\"\n",
        "        \"Provide the correct answer and a brief explanation.\"\n",
        "    )\n",
        "\n",
        "    # Build assistant output\n",
        "    assistant_output = (\n",
        "        f\"The correct answer is: {correct}.\\n\"\n",
        "        f\"Explanation: {explanation}\"\n",
        "    )\n",
        "\n",
        "    # TinyLlama chat format\n",
        "    text = f\"<s>[INST] {user_prompt} [/INST] {assistant_output}</s>\"\n",
        "\n",
        "    return {\"text\": text}\n"
      ],
      "metadata": {
        "id": "FrCJVgQaxvEC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nüìä Loading dataset...\")\n",
        "dataset = load_dataset(\"openlifescienceai/medmcqa\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFEMtdEEyd1i",
        "outputId": "67d74edd-b5e4-4ee7-aa95-805eab68f253"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Loading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWS4zwmV0HL4",
        "outputId": "d5e10208-74d6-41bd-ba15-3987241eb9b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
              "        num_rows: 182822\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
              "        num_rows: 6150\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
              "        num_rows: 4183\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset = dataset[\"train\"].map(format_prompt)\n",
        "\n",
        "formatted_dataset = formatted_dataset.filter(\n",
        "    lambda x: x[\"text\"] is not None\n",
        ")\n",
        "\n",
        "formatted_dataset[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921,
          "referenced_widgets": [
            "eeefb5fd0cc34349bf3afd459a3c5a48",
            "30091f80067749949f9f1a6cde80ff26",
            "f5e109e7ef2646e3b3750693b1e84342",
            "af95de76810444eb8f7ff15e3d424c8d",
            "f8ce81f18d114ed78ff7948d8b3c1981",
            "40caa860441d47b99636693a5fc720e9",
            "ea670a871bfe42d69aff5756b8dc4f38",
            "48cdd74bb5c94ca6a3a5faed495d950f",
            "788f193f3ba242de91c6d6b366db3346",
            "29913a3d3013414cafad797fe55d7db0",
            "b49843b2579e48448539fc1afc06f4e2",
            "e0d06d300e974949b82a7d4aa12d7342",
            "9c9ba36eee3a4e5e977a9abcbcc8143c",
            "45b5b7e6829242f29217e70708fd1524",
            "1cc15feae01c46c6877d95fe0fd52b66",
            "eccac10aa2914da5bfe5202a92f72870",
            "d5d057242c3d48a3ba572d9919ab6786",
            "08bc9611efbe4b0abdf2f99ffee4f8b2",
            "34cbf73abe6149c3b84c1b076be78481",
            "0f7d6da4441d4fed8e5da7f1901408b4",
            "b5053d2764c242ef960208688e56c545",
            "1596ba73b19a4cddb4598d2908f9fd6a"
          ]
        },
        "id": "7ulS3sTOzHLy",
        "outputId": "82d464b6-0ced-4ce4-ca29-7b45a4601b6e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/182822 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eeefb5fd0cc34349bf3afd459a3c5a48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/182822 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0d06d300e974949b82a7d4aa12d7342"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'e9ad821a-c438-4965-9f77-760819dfa155',\n",
              " 'question': 'Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma',\n",
              " 'opa': 'Hyperplasia',\n",
              " 'opb': 'Hyperophy',\n",
              " 'opc': 'Atrophy',\n",
              " 'opd': 'Dyplasia',\n",
              " 'cop': 2,\n",
              " 'choice_type': 'single',\n",
              " 'exp': 'Chronic urethral obstruction because of urinary calculi, prostatic hyperophy, tumors, normal pregnancy, tumors, uterine prolapse or functional disorders cause hydronephrosis which by definition is used to describe dilatation of renal pelvis and calculus associated with progressive atrophy of the kidney due to obstruction to the outflow of urine Refer Robbins 7yh/9,1012,9/e. P950',\n",
              " 'subject_name': 'Anatomy',\n",
              " 'topic_name': 'Urinary tract',\n",
              " 'text': '<s>[INST] You are a medical assistant. Answer the following multiple-choice medical question.\\n\\nQuestion: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma\\nA: Hyperplasia\\nB: Hyperophy\\nC: Atrophy\\nD: Dyplasia\\n\\nProvide the correct answer and a brief explanation. [/INST] The correct answer is: C.\\nExplanation: Chronic urethral obstruction because of urinary calculi, prostatic hyperophy, tumors, normal pregnancy, tumors, uterine prolapse or functional disorders cause hydronephrosis which by definition is used to describe dilatation of renal pelvis and calculus associated with progressive atrophy of the kidney due to obstruction to the outflow of urine Refer Robbins 7yh/9,1012,9/e. P950</s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = formatted_dataset.select(range(500))\n"
      ],
      "metadata": {
        "id": "2M-OhFnEzO4H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training on {len(train_dataset)} examples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kgr2I6xzXM-",
        "outputId": "fcef7001-7fd5-4c43-d855-1c45d0dad31a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 500 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Labels = copy of input_ids for causal LM\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n"
      ],
      "metadata": {
        "id": "-AvmPCe4zpcl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.column_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfpI04tVz5IM",
        "outputId": "2f65fe54-4af2-4b2c-ba44-f4f1714cbfc6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'question',\n",
              " 'opa',\n",
              " 'opb',\n",
              " 'opc',\n",
              " 'opd',\n",
              " 'cop',\n",
              " 'choice_type',\n",
              " 'exp',\n",
              " 'subject_name',\n",
              " 'topic_name',\n",
              " 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üî§ Tokenizing...\")\n",
        "\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "tokenized_train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "095424f7252c4b3db28c401539ca84b8",
            "ad88c93bb95140b0986f8d20a11014b0",
            "0037e9ed7b294866b1b08130e62c7b98",
            "5deaeb3afb1d4174bccee22723bbe2e4",
            "3c67d1a867814c6796f50349962ea715",
            "1d0c32c5892848a180264ebafd71af63",
            "bf6f7806b8ae4feb88672901b5ed6f5e",
            "5c10b1e74c5949d0bf3b7f6e9f3b47b5",
            "74cc32161c954a71a853b4009f2c9f0a",
            "951214ee67bb412abb0d01fad9aad3c8",
            "d6b5be51b20c422b9cda5a35d4ac21bb"
          ]
        },
        "id": "bEbJNsYnzupg",
        "outputId": "daf89316-b74f-4c1e-d121-0148659ca858"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Tokenizing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "095424f7252c4b3db28c401539ca84b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results-tinyllama-medmcqa\",\n",
        "    num_train_epochs=2,                       # Enough for 500 samples\n",
        "    per_device_train_batch_size=4,            # Works on T4/L4\n",
        "    gradient_accumulation_steps=4,            # Effective batch size = 16\n",
        "    learning_rate=2e-4,                       # Standard for LoRA\n",
        "    warmup_steps=10,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,                                # IMPORTANT on CUDA\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",                         # No wandb\n",
        ")\n"
      ],
      "metadata": {
        "id": "Wl17opVb17I6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "EbfDaJPc2YBl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ],
      "metadata": {
        "id": "m-7lHqg-2azs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(\"=\"*60)\n",
        "trainer.train()\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "Et-UZ8pY2cpo",
        "outputId": "84efb742-bd4a-479b-fcac-5b94a0b1ecd3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 03:36, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.239200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.714600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.612700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.561800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.481200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=64, training_loss=1.6908948570489883, metrics={'train_runtime': 221.8612, 'train_samples_per_second': 4.507, 'train_steps_per_second': 0.288, 'total_flos': 3216777412608000.0, 'train_loss': 1.6908948570489883, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüíæ Saving model...\")\n",
        "\n",
        "model.save_pretrained(\"./tinyllama_medmcqa_lora\")\n",
        "tokenizer.save_pretrained(\"./tinyllama_medmcqa_lora\")\n",
        "\n",
        "print(\"‚úÖ Model saved to ./tinyllama_medmcqa_lora\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Sn9rSU3prN",
        "outputId": "432b937a-d3e5-4cd2-ce7d-c0acbc11890c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Saving model...\n",
            "‚úÖ Model saved to ./tinyllama_medmcqa_lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1"
      ],
      "metadata": {
        "id": "SHrtuBRi5ugL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"üì• Loading full MedMCQA dataset...\")\n",
        "\n",
        "dataset = load_dataset(\"openlifescienceai/medmcqa\")[\"train\"]\n",
        "\n",
        "print(\"Total dataset size:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9ECnkgC4rlX",
        "outputId": "79fc5e5e-7adc-4f43-ae40-87167fbf88d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Loading full MedMCQA dataset...\n",
            "Total dataset size: 182822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = dataset.select(range(0, 1000))       # Used for fine-tuning\n",
        "test_set  = dataset.select(range(1000, len(dataset)))  # Unseen examples\n"
      ],
      "metadata": {
        "id": "PcXPbgBt5xrr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train set size:\", len(train_set))\n",
        "print(\"Test set size:\", len(test_set))\n",
        "\n",
        "print(\"\\nüîé First train example:\")\n",
        "print(train_set[0])\n",
        "\n",
        "print(\"\\nüîé First test example:\")\n",
        "print(test_set[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klifqR7k5zHr",
        "outputId": "87e2a1b5-cffd-41c0-94a3-ca98187a3b80"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 1000\n",
            "Test set size: 181822\n",
            "\n",
            "üîé First train example:\n",
            "{'id': 'e9ad821a-c438-4965-9f77-760819dfa155', 'question': 'Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma', 'opa': 'Hyperplasia', 'opb': 'Hyperophy', 'opc': 'Atrophy', 'opd': 'Dyplasia', 'cop': 2, 'choice_type': 'single', 'exp': 'Chronic urethral obstruction because of urinary calculi, prostatic hyperophy, tumors, normal pregnancy, tumors, uterine prolapse or functional disorders cause hydronephrosis which by definition is used to describe dilatation of renal pelvis and calculus associated with progressive atrophy of the kidney due to obstruction to the outflow of urine Refer Robbins 7yh/9,1012,9/e. P950', 'subject_name': 'Anatomy', 'topic_name': 'Urinary tract'}\n",
            "\n",
            "üîé First test example:\n",
            "{'id': 'e0c2cf58-aa5c-4516-b250-79b76d99a2cc', 'question': 'Drug of choice for OCD is?', 'opa': 'Clomipramine', 'opb': 'Fluoxetine', 'opc': 'Carbamezapine', 'opd': 'Chlorpromazine', 'cop': 1, 'choice_type': 'single', 'exp': 'ANSWER: (B) FluoxetineREF: Kaplan 9th ed p - 622OCDTreatment of choice: behaviour therapyDrug of choice: SSRI2nd drug of choice: comipramine', 'subject_name': 'Psychiatry', 'topic_name': 'Neurotic Disorders'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2"
      ],
      "metadata": {
        "id": "8pFr4JIF5-KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# 1. Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# 2. Randomly select 20 indices from the test set\n",
        "num_test_samples = 20\n",
        "selected_indices = random.sample(range(len(test_set)), num_test_samples)\n",
        "\n",
        "# 3. Record indices and extract examples\n",
        "print(\"üß™ Selected test indices:\", selected_indices)\n",
        "\n",
        "test_samples = [test_set[i] for i in selected_indices]\n",
        "\n",
        "print(f\"Sampled {len(test_samples)} test examples successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRl3F4jb6Hiw",
        "outputId": "998bf5b7-e0a8-4994-bb16-a120ecd63a8e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Selected test indices: [167621, 29184, 6556, 72097, 64196, 58513, 36579, 26868, 177392, 142964, 22790, 154794, 110604, 8331, 7811, 24561, 57314, 60990, 132475, 157815]\n",
            "Sampled 20 test examples successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3"
      ],
      "metadata": {
        "id": "b2tYbPDx6UVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(example):\n",
        "    # 1. Format prompt using TinyLlama chat template\n",
        "    prompt = (\n",
        "        \"<s>[INST] You are a medical assistant.\\n\\n\"\n",
        "        f\"Question: {example['question']}\\n\"\n",
        "        f\"A: {example['opa']}\\n\"\n",
        "        f\"B: {example['opb']}\\n\"\n",
        "        f\"C: {example['opc']}\\n\"\n",
        "        f\"D: {example['opd']}\\n\\n\"\n",
        "        \"Provide the correct answer and a brief explanation. [/INST]\"\n",
        "    )\n",
        "\n",
        "    # 2. Tokenize and move to device (GPU)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 3. Generate model prediction\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            temperature=0.3,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "    # 4. Decode and return only the assistant response\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove everything before the assistant reply\n",
        "    response = decoded.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "z-ZNvzod6TzP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5"
      ],
      "metadata": {
        "id": "UCmU2rAn6i2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Common stopwords to ignore in partial matching\n",
        "STOPWORDS = {\n",
        "    \"the\",\"a\",\"and\",\"of\",\"to\",\"in\",\"for\",\"on\",\"with\",\"that\",\"is\",\"it\",\"as\",\"are\",\n",
        "    \"an\",\"by\",\"from\",\"this\",\"these\",\"those\",\"be\",\"or\",\"at\",\"into\",\"its\",\"was\"\n",
        "}\n",
        "\n",
        "def normalize(text):\n",
        "    \"\"\"Lowercase, remove punctuation, split into words.\"\"\"\n",
        "    return re.sub(r\"[^a-zA-Z0-9 ]\", \"\", text.lower()).split()\n",
        "\n",
        "def partial_match(pred, truth, threshold=0.7):\n",
        "    \"\"\"Return True if ‚â•70% of non-stopword truth terms appear in the prediction.\"\"\"\n",
        "    pred_words  = [w for w in normalize(pred)  if w not in STOPWORDS]\n",
        "    truth_words = [w for w in normalize(truth) if w not in STOPWORDS]\n",
        "\n",
        "    if not truth_words:\n",
        "        return False\n",
        "\n",
        "    overlap = sum(1 for w in truth_words if w in pred_words)\n",
        "    ratio = overlap / len(truth_words)\n",
        "\n",
        "    return ratio >= threshold\n",
        "\n",
        "\n",
        "def check_accuracy(prediction, example):\n",
        "    \"\"\"\n",
        "    Compare model prediction to ground truth using exact and partial matching.\n",
        "    Returns: \"exact\", \"partial\", or \"wrong\"\n",
        "    \"\"\"\n",
        "    # 1. Find correct answer (A/B/C/D)\n",
        "    index_to_letter = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
        "    correct_letter  = index_to_letter[example[\"cop\"]]\n",
        "\n",
        "    correct_text = example[f\"op{correct_letter.lower()}\"]\n",
        "\n",
        "    # 2. Exact match : if correct letter OR exact text appears in prediction\n",
        "    if correct_letter in prediction or correct_text.lower() in prediction.lower():\n",
        "        return \"exact\"\n",
        "\n",
        "    # 3. Partial match : semantic similarity on medical keywords\n",
        "    if partial_match(prediction, correct_text):\n",
        "        return \"partial\"\n",
        "\n",
        "    # 4. Else ‚Üí incorrect\n",
        "    return \"wrong\"\n"
      ],
      "metadata": {
        "id": "7YCU04Ks6iKN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5"
      ],
      "metadata": {
        "id": "F_KX3HL77Wwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "print(\"\\nüöÄ Starting evaluation on 20 samples...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "exact = 0\n",
        "partial = 0\n",
        "wrong = 0\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx, example in zip(selected_indices, test_samples):\n",
        "\n",
        "    print(f\"\\n--- üß™ Test Example (index {idx}) ---\")\n",
        "\n",
        "    # 1. Extract question (truncate if long)\n",
        "    question_preview = example[\"question\"][:200]\n",
        "    print(f\"Question: {question_preview}...\")\n",
        "\n",
        "    # 2. Generate prediction\n",
        "    prediction = get_prediction(example)\n",
        "    print(f\"\\nüß† Model prediction:\\n{prediction}\\n\")\n",
        "\n",
        "    # 3. Evaluate accuracy\n",
        "    acc_type = check_accuracy(prediction, example)\n",
        "\n",
        "    if acc_type == \"exact\":\n",
        "        print(\"‚úÖ Exact match\")\n",
        "        exact += 1\n",
        "    elif acc_type == \"partial\":\n",
        "        print(\"üü° Partial match\")\n",
        "        partial += 1\n",
        "    else:\n",
        "        print(\"‚ùå Incorrect\")\n",
        "        wrong += 1\n",
        "\n",
        "    # 4. Save detailed result\n",
        "    index_to_letter = {0:\"A\",1:\"B\",2:\"C\",3:\"D\"}\n",
        "    correct_letter = index_to_letter[example[\"cop\"]]\n",
        "    correct_text = example[f\"op{correct_letter.lower()}\"]\n",
        "\n",
        "    results.append({\n",
        "        \"index\": idx,\n",
        "        \"question\": example[\"question\"],\n",
        "        \"prediction\": prediction,\n",
        "        \"correct_letter\": correct_letter,\n",
        "        \"correct_text\": correct_text,\n",
        "        \"accuracy_type\": acc_type\n",
        "    })\n",
        "\n",
        "elapsed = time.time() - start_time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wayCqu2X7Y0u",
        "outputId": "a37bca0f-bf18-41cc-90a7-1569ae1e4562"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting evaluation on 20 samples...\n",
            "\n",
            "\n",
            "--- üß™ Test Example (index 167621) ---\n",
            "Question: Which of the following is found in the respiratory zone of the lung?...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Goblet cells. Ref: 10th edition, Page no. 1022. Ref: 10th\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 29184) ---\n",
            "Question: Which of the following does not occur in starvation?...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Hypoglycemia. Ref: 10th edition, Page no. 1028, 1029\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 6556) ---\n",
            "Question: A 20 month old female child is brought for routine check-up. Complete blood count (CBC) shows moderate neutropenia. Child looks healthy, eats well and within expected parameters for age and sex. Other...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Corticosteroid administration. Ref: 10th edition, Page 1032. Ref: 10th\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 72097) ---\n",
            "Question: Urgent treatment of procainamide toxicity is:...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: B.\n",
            "Explanation: Procainamide is a potent inhibitor of the Na+/K+ ATPase pump. It is used in the treatment of ventricular fibrillation\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 64196) ---\n",
            "Question: Which of the following agents is not used in the treatment of Diabetic Macular Edema Retinopathy-...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: 1. Pyridazinones 2. Benfotiamine 3. Tamoxifen 4. Ruboxistaurim 5. Diltiazem\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 58513) ---\n",
            "Question: Mutation in alpha 5 chain of collagen 4, the diagonis is -...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: B.\n",
            "Explanation: 1. Alpo's syndrome 2. Thin membrane disease 3. Good pasture syndrome 4. Nodular glomerulosclerosis\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 36579) ---\n",
            "Question: Which of the following is not done for diagnosis of parotid tumor...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., MRI. MRI is the most sensitive and specific imaging modality for diagnosis of parotid tumors. It is a\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 26868) ---\n",
            "Question: Congenital Infection affecting the fetus with minimal teratogenic risk is:...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: 1. Congenital Infection affecting the fetus with minimal teratogenic risk is: A. HIV B. Rubella C. Varicella D. C\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 177392) ---\n",
            "Question: Breast carcinoma is associated with all except:...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., BRCA1. Ref: 10th edition of the AMA guidelines for medical assistant. 10th edition of\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 142964) ---\n",
            "Question: For the following statements, select the most likely type of Hodgkin disease (HD).This variant has a particularly good outcome....\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: 1. lymphocyte-predominant Hodgkin disease (HD) 2. nodular-sclerosing HD 3. mixed-cellular\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 22790) ---\n",
            "Question: A 24 year old male presented with retroperitoneal left necrotic mass near the hilum of kidney which showed heterogenous contrast enhancement on CECT. What is the probable diagnosis?...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: C.\n",
            "Explanation: Metastatic transitional cell carcinoma (MTC) is a rare type of bladder cancer. It is a malignant tumor of the transitional cells of the\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 154794) ---\n",
            "Question: Polycythemia is seen with which tumor -...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Renal cell carcinoma Polycythemia is seen with renal cell carcinoma. Renal cell carcinoma is the\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 110604) ---\n",
            "Question: Incidence of which lymphoma is MORE common in females?...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Mantle cell lymphoma. Ref: 10th edition, Page 1033, 10th edition\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 8331) ---\n",
            "Question: A 29-year-old female diagnosed with AIDS has been suffering from a progressive blurring of vision in her right eye. On fundoscopic examination, a small white opaque lesion is noted on the retina of he...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'A' i.e., Acyclovir Ref: 10th edition, Page 1022, 10th edition, Page 1\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 7811) ---\n",
            "Question: Death of Poliomyelitis is due to -...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Infection. Ref: 10th edition, Page no. 1037, 1038, 1\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 24561) ---\n",
            "Question: Xenon anesthesia all are true except...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Slow induction and recovery. Ans. is 'a' i.e., Slow induction and recovery. Ans. is 'a'\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 57314) ---\n",
            "Question: Thyroid hormones in blood is transpoed by:...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: 1. Albumin is the major carrier of thyroid hormones in blood. 2. Albumin is a globular protein that is synthesized in the liver\n",
            "\n",
            "‚ùå Incorrect\n",
            "\n",
            "--- üß™ Test Example (index 60990) ---\n",
            "Question: Kernohan's notch is seen in the following organ injury:...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'A' i.e., Brain Kernohan's notch is seen in the brain. Ref: 10th edition, Page 10\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 132475) ---\n",
            "Question: True about bipolar disorder type II is-...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: C.\n",
            "Explanation: Ans. is 'c' i.e., Repetitive depression & hypomania Ref: 10th edition, Page 1022, 102\n",
            "\n",
            "‚úÖ Exact match\n",
            "\n",
            "--- üß™ Test Example (index 157815) ---\n",
            "Question: Lynch Howah surgery is for:...\n",
            "\n",
            "üß† Model prediction:\n",
            "The correct answer is: C.\n",
            "Explanation: Ans. is 'c' i.e., Acoustic neuroma. Ref: 10th edition, Page 1003, 1004,\n",
            "\n",
            "‚ùå Incorrect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6"
      ],
      "metadata": {
        "id": "NeqCoJyu7qjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of evaluated examples\n",
        "total = len(results)\n",
        "\n",
        "print(\"\\n===================== üìä FINAL METRICS üìä =====================\")\n",
        "\n",
        "# Exact matches\n",
        "exact_pct = (exact / total) * 100\n",
        "\n",
        "# Partial matches\n",
        "partial_pct = (partial / total) * 100\n",
        "\n",
        "# Overall accuracy (exact + partial)\n",
        "overall_accuracy = ((exact + partial) / total) * 100\n",
        "\n",
        "# Average time per example\n",
        "avg_time = elapsed / total\n",
        "\n",
        "print(f\"Total examples evaluated: {total}\")\n",
        "print(f\"Exact matches: {exact}  ({exact_pct:.1f}%)\")\n",
        "print(f\"Partial matches: {partial}  ({partial_pct:.1f}%)\")\n",
        "print(f\"Incorrect predictions: {wrong}\")\n",
        "print(f\"\\nüéØ Overall accuracy: {overall_accuracy:.1f}%\")\n",
        "print(f\"\\n‚è±Ô∏è Total evaluation time: {elapsed:.2f} seconds\")\n",
        "print(f\"‚è±Ô∏è Average time per example: {avg_time:.2f} seconds\")\n",
        "\n",
        "print(\"===============================================================\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx276reY7r4d",
        "outputId": "88ed90cf-8862-4543-8223-ab5e52049043"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== üìä FINAL METRICS üìä =====================\n",
            "Total examples evaluated: 20\n",
            "Exact matches: 10  (50.0%)\n",
            "Partial matches: 0  (0.0%)\n",
            "Incorrect predictions: 10\n",
            "\n",
            "üéØ Overall accuracy: 50.0%\n",
            "\n",
            "‚è±Ô∏è Total evaluation time: 67.62 seconds\n",
            "‚è±Ô∏è Average time per example: 3.38 seconds\n",
            "===============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7"
      ],
      "metadata": {
        "id": "KrnVkel972Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n‚ùå‚ùå‚ùå INCORRECT EXAMPLES ‚ùå‚ùå‚ùå\")\n",
        "\n",
        "wrong_examples = [r for r in results if r[\"accuracy_type\"] == \"wrong\"]\n",
        "\n",
        "print(f\"\\nTotal incorrect: {len(wrong_examples)}\")\n",
        "\n",
        "for r in wrong_examples:\n",
        "    print(\"\\n--------------------------------------------------\")\n",
        "    print(f\"Index: {r['index']}\")\n",
        "    print(f\"Question: {r['question']}\")\n",
        "    print(f\"Correct answer: {r['correct_letter']} ‚Äî {r['correct_text']}\")\n",
        "    print(f\"Model prediction:\\n{r['prediction']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWKzXs3171us",
        "outputId": "23967289-07cb-4681-d140-bc4e2a85e935"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùå‚ùå‚ùå INCORRECT EXAMPLES ‚ùå‚ùå‚ùå\n",
            "\n",
            "Total incorrect: 10\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 167621\n",
            "Question: Which of the following is found in the respiratory zone of the lung?\n",
            "Correct answer: D ‚Äî Type I epithelial cells\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Goblet cells. Ref: 10th edition, Page no. 1022. Ref: 10th\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 29184\n",
            "Question: Which of the following does not occur in starvation?\n",
            "Correct answer: B ‚Äî Hypercholesterolemia\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Hypoglycemia. Ref: 10th edition, Page no. 1028, 1029\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 72097\n",
            "Question: Urgent treatment of procainamide toxicity is:\n",
            "Correct answer: D ‚Äî Sodium lactate\n",
            "Model prediction:\n",
            "The correct answer is: B.\n",
            "Explanation: Procainamide is a potent inhibitor of the Na+/K+ ATPase pump. It is used in the treatment of ventricular fibrillation\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 36579\n",
            "Question: Which of the following is not done for diagnosis of parotid tumor\n",
            "Correct answer: C ‚Äî Open surgical biopsy\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., MRI. MRI is the most sensitive and specific imaging modality for diagnosis of parotid tumors. It is a\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 177392\n",
            "Question: Breast carcinoma is associated with all except:\n",
            "Correct answer: D ‚Äî ATR\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., BRCA1. Ref: 10th edition of the AMA guidelines for medical assistant. 10th edition of\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 110604\n",
            "Question: Incidence of which lymphoma is MORE common in females?\n",
            "Correct answer: B ‚Äî Follicular lymphoma\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Mantle cell lymphoma. Ref: 10th edition, Page 1033, 10th edition\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 8331\n",
            "Question: A 29-year-old female diagnosed with AIDS has been suffering from a progressive blurring of vision in her right eye. On fundoscopic examination, a small white opaque lesion is noted on the retina of her right eye. Most appropriate therapy for this patient?\n",
            "Correct answer: D ‚Äî Ganciclovir\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'A' i.e., Acyclovir Ref: 10th edition, Page 1022, 10th edition, Page 1\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 7811\n",
            "Question: Death of Poliomyelitis is due to -\n",
            "Correct answer: D ‚Äî Respiratory paralysis\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Infection. Ref: 10th edition, Page no. 1037, 1038, 1\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 57314\n",
            "Question: Thyroid hormones in blood is transpoed by:\n",
            "Correct answer: D ‚Äî All\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: 1. Albumin is the major carrier of thyroid hormones in blood. 2. Albumin is a globular protein that is synthesized in the liver\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 157815\n",
            "Question: Lynch Howah surgery is for:\n",
            "Correct answer: B ‚Äî Sinonasal tumours\n",
            "Model prediction:\n",
            "The correct answer is: C.\n",
            "Explanation: Ans. is 'c' i.e., Acoustic neuroma. Ref: 10th edition, Page 1003, 1004,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n‚úÖ‚úÖ‚úÖ SAMPLE OF CORRECT EXAMPLES (first 5) ‚úÖ‚úÖ‚úÖ\")\n",
        "\n",
        "correct_examples = [r for r in results if r[\"accuracy_type\"] in [\"exact\", \"partial\"]]\n",
        "\n",
        "print(f\"\\nTotal correct: {len(correct_examples)}\")\n",
        "print(\"\\nShowing first 5 correct examples:\\n\")\n",
        "\n",
        "for r in correct_examples[:5]:\n",
        "    print(\"\\n--------------------------------------------------\")\n",
        "    print(f\"Index: {r['index']}\")\n",
        "    print(f\"Question: {r['question']}\")\n",
        "    print(f\"Correct answer: {r['correct_letter']} ‚Äî {r['correct_text']}\")\n",
        "    print(f\"Model prediction:\\n{r['prediction']}\")\n",
        "    print(f\"Match type: {r['accuracy_type']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0d2rocP790U",
        "outputId": "8d1386c4-8358-4f2c-d889-10a0954dac73"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "‚úÖ‚úÖ‚úÖ SAMPLE OF CORRECT EXAMPLES (first 5) ‚úÖ‚úÖ‚úÖ\n",
            "\n",
            "Total correct: 10\n",
            "\n",
            "Showing first 5 correct examples:\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 6556\n",
            "Question: A 20 month old female child is brought for routine check-up. Complete blood count (CBC) shows moderate neutropenia. Child looks healthy, eats well and within expected parameters for age and sex. Other parameters of blood count are within normal range expected for age. Family history is unremarkable. CBC after 1 and 2 weeks shows same results. Bone marrow examination is normal. Next step\n",
            "Correct answer: C ‚Äî Watch and wait strategy\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: Ans. is 'a' i.e., Corticosteroid administration. Ref: 10th edition, Page 1032. Ref: 10th\n",
            "Match type: exact\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 64196\n",
            "Question: Which of the following agents is not used in the treatment of Diabetic Macular Edema Retinopathy-\n",
            "Correct answer: D ‚Äî Tamoxifen\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: 1. Pyridazinones 2. Benfotiamine 3. Tamoxifen 4. Ruboxistaurim 5. Diltiazem\n",
            "Match type: exact\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 58513\n",
            "Question: Mutation in alpha 5 chain of collagen 4, the diagonis is -\n",
            "Correct answer: A ‚Äî Alpo's syndrome\n",
            "Model prediction:\n",
            "The correct answer is: B.\n",
            "Explanation: 1. Alpo's syndrome 2. Thin membrane disease 3. Good pasture syndrome 4. Nodular glomerulosclerosis\n",
            "Match type: exact\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 26868\n",
            "Question: Congenital Infection affecting the fetus with minimal teratogenic risk is:\n",
            "Correct answer: A ‚Äî HIV\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: 1. Congenital Infection affecting the fetus with minimal teratogenic risk is: A. HIV B. Rubella C. Varicella D. C\n",
            "Match type: exact\n",
            "\n",
            "--------------------------------------------------\n",
            "Index: 142964\n",
            "Question: For the following statements, select the most likely type of Hodgkin disease (HD).This variant has a particularly good outcome.\n",
            "Correct answer: A ‚Äî lymphocyte-predominant Hodgkin disease (HD)\n",
            "Model prediction:\n",
            "The correct answer is: A.\n",
            "Explanation: 1. lymphocyte-predominant Hodgkin disease (HD) 2. nodular-sclerosing HD 3. mixed-cellular\n",
            "Match type: exact\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 8"
      ],
      "metadata": {
        "id": "k3W77lts8O3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===================== üß† PERFORMANCE ASSESSMENT üß† =====================\")\n",
        "\n",
        "acc = overall_accuracy  # from step 6\n",
        "\n",
        "if acc >= 80:\n",
        "    assessment = (\n",
        "        \"üåü EXCELLENT PERFORMANCE\\n\"\n",
        "        \"Your model achieved ‚â•80% accuracy.\\n\"\n",
        "        \"‚Üí Fine-tuning was highly successful.\\n\"\n",
        "        \"‚Üí The model generalizes very well to unseen medical questions.\\n\"\n",
        "        \"‚Üí You likely chose good hyperparameters and data formatting.\"\n",
        "    )\n",
        "\n",
        "elif acc >= 60:\n",
        "    assessment = (\n",
        "        \"‚úÖ GOOD PERFORMANCE\\n\"\n",
        "        \"Your model achieved between 60‚Äì79% accuracy.\\n\"\n",
        "        \"‚Üí The model learned successfully.\\n\"\n",
        "        \"‚Üí Minor improvements (more training data or more epochs) could push it higher.\"\n",
        "    )\n",
        "\n",
        "elif acc >= 40:\n",
        "    assessment = (\n",
        "        \"üü° MODERATE PERFORMANCE\\n\"\n",
        "        \"Your model achieved between 40‚Äì59% accuracy.\\n\"\n",
        "        \"‚Üí This is okay, but the model may struggle with nuance.\\n\"\n",
        "        \"‚Üí Consider: training longer, using more high-quality samples, improving formatting.\"\n",
        "    )\n",
        "\n",
        "elif acc >= 20:\n",
        "    assessment = (\n",
        "        \"‚ö†Ô∏è POOR PERFORMANCE\\n\"\n",
        "        \"Your model achieved between 20‚Äì39% accuracy.\\n\"\n",
        "        \"‚Üí Something is off: the dataset, LoRA config, or training duration.\\n\"\n",
        "        \"‚Üí Investigate formatting, cleaning, or using a larger dataset.\"\n",
        "    )\n",
        "\n",
        "else:\n",
        "    assessment = (\n",
        "        \"‚ùå VERY POOR PERFORMANCE\\n\"\n",
        "        \"Your model achieved <20% accuracy.\\n\"\n",
        "        \"‚Üí The model likely didn't learn anything meaningful.\\n\"\n",
        "        \"‚Üí Verify: training loop, dataset format, prompt template, LoRA parameters.\"\n",
        "    )\n",
        "\n",
        "print(assessment)\n",
        "print(\"=======================================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ3Fe7rV8R3B",
        "outputId": "6b249e9e-0f87-4fdd-fd03-13180c452ce4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================== üß† PERFORMANCE ASSESSMENT üß† =====================\n",
            "üü° MODERATE PERFORMANCE\n",
            "Your model achieved between 40‚Äì59% accuracy.\n",
            "‚Üí This is okay, but the model may struggle with nuance.\n",
            "‚Üí Consider: training longer, using more high-quality samples, improving formatting.\n",
            "=======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 9"
      ],
      "metadata": {
        "id": "VLvZfU988eEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Build results dictionary\n",
        "evaluation_output = {\n",
        "    \"metrics\": {\n",
        "        \"total_evaluated\": total,\n",
        "        \"exact_matches\": exact,\n",
        "        \"partial_matches\": partial,\n",
        "        \"incorrect\": wrong,\n",
        "        \"exact_match_percentage\": exact_pct,\n",
        "        \"partial_match_percentage\": partial_pct,\n",
        "        \"overall_accuracy_percentage\": overall_accuracy,\n",
        "        \"total_time_seconds\": elapsed,\n",
        "        \"average_time_per_example_seconds\": avg_time,\n",
        "    },\n",
        "    \"selected_test_indices\": selected_indices,\n",
        "    \"detailed_results\": results\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "output_filename = \"evaluation_results.json\"\n",
        "\n",
        "with open(output_filename, \"w\") as f:\n",
        "    json.dump(evaluation_output, f, indent=4)\n",
        "\n",
        "print(f\"\\nüíæ Results successfully saved to {output_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU3rb02y8fb6",
        "outputId": "91d65b29-b247-4a71-9f90-598ba1118c83"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Results successfully saved to evaluation_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part A : Model Improvement Strategies"
      ],
      "metadata": {
        "id": "0uTAHDb4-XTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1: Improving Model Performance\n",
        "1. Increase Training Data\n",
        "\n",
        "Change: Train on more than 1000 samples (e.g., 5k‚Äì20k).\n",
        "Why: More examples ‚Üí better generalization and fewer random errors.\n",
        "Trade-off: Longer training time and higher GPU usage.\n",
        "\n",
        "2. Improve Prompt Format\n",
        "\n",
        "Change: Use a clearer instruction template (force model to pick A/B/C/D).\n",
        "Why: Reduces hallucinations and increases exact matches.\n",
        "Trade-off: Must retrain the model with the new format for consistency.\n",
        "\n",
        "3. Strengthen LoRA Fine-Tuning\n",
        "\n",
        "Change: Increase LoRA rank or use QLoRA.\n",
        "Why: Gives the model more capacity to learn medical patterns.\n",
        "Trade-off: Slightly slower training and higher risk of overfitting."
      ],
      "metadata": {
        "id": "emxJEygx-vit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 2: Analyzing Failure Patterns\n",
        "\n",
        "By looking at the incorrect predictions, several patterns appear:\n",
        "\n",
        "1. Confusion Between Similar Options\n",
        "\n",
        "The model often fails when two or more choices are very close (e.g. two similar drugs or diagnoses).\n",
        "It seems to pick a plausible answer, but not always the most specific or guideline-consistent one.\n",
        "\n",
        "2. Weak Multi-Step Reasoning\n",
        "\n",
        "Errors are frequent on questions that require several reasoning steps (symptoms ‚Üí mechanism ‚Üí treatment).\n",
        "The model tends to rely on surface associations (keywords) instead of fully chaining the reasoning.\n",
        "\n",
        "3. Sensitivity to Wording and Context\n",
        "\n",
        "When the question is long, complex, or includes subtle clinical details, the model sometimes ignores key modifiers (e.g. ‚Äúacute vs chronic‚Äù, ‚Äúchild vs adult‚Äù), leading to a wrong but superficially reasonable answer.\n",
        "\n",
        "4. Hallucinated or Over-Explained Answers\n",
        "\n",
        "In some failures, the model gives a confident explanation that does not match any option correctly.\n",
        "This suggests that it sometimes hallucinates a generic medical answer instead of strictly choosing among A/B/C/D."
      ],
      "metadata": {
        "id": "QIVDGcla-_V9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3: Data Quality vs. Quantity\n",
        "\n",
        "Between 2000 standard-quality examples and 500 curated high-quality examples, the better choice depends on the task ‚Äî but for medical question-answering, 500 high-quality examples are usually more valuable.\n",
        "\n",
        "Why high-quality data is better\n",
        "\n",
        "High-quality samples have clear structure, consistent formatting, and accurate explanations, which the model can learn from reliably.\n",
        "\n",
        "Medical reasoning is sensitive to noise; low-quality samples can introduce incorrect associations that harm performance.\n",
        "\n",
        "Curated examples cover concepts more deliberately, improving generalization.\n",
        "\n",
        "When quantity helps more\n",
        "\n",
        "If the model needs broad coverage of many topics\n",
        "\n",
        "If the dataset is clean enough and not too noisy"
      ],
      "metadata": {
        "id": "XMLa4tb0_MF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B : Resource-Constrained Inference"
      ],
      "metadata": {
        "id": "NgvYYRbz_Ro-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4: Optimizing for Limited Resources\n",
        "\n",
        "When deploying a model in constrained environments (low memory, low latency, edge devices), the goal is to reduce inference time and memory while keeping accuracy acceptable. A good strategy combines several techniques:\n",
        "\n",
        "1. Quantization (4-bit or 8-bit)\n",
        "\n",
        "Idea: Convert model weights from float16/32 to int8 or int4.\n",
        "Why it helps:\n",
        "\n",
        "Shrinks model size by 50‚Äì75%\n",
        "\n",
        "Speeds up inference significantly\n",
        "Trade-off:\n",
        "\n",
        "Slight loss in precision, especially for complex tasks\n",
        "\n",
        "2. Use a Smaller Base Model\n",
        "\n",
        "Idea: Deploy TinyLlama (1.1B) or even 0.5B versions instead of larger LLaMA models.\n",
        "Why it helps:\n",
        "\n",
        "Less memory, faster inference\n",
        "Trade-off:\n",
        "\n",
        "Lower reasoning ability compared to larger models\n",
        "\n",
        "3. Distillation\n",
        "\n",
        "Idea: Train a smaller ‚Äústudent‚Äù model to mimic a larger ‚Äúteacher‚Äù model.\n",
        "Why it helps:\n",
        "\n",
        "Maintains much of the big model‚Äôs performance\n",
        "\n",
        "Runs far faster on edge devices\n",
        "Trade-off:\n",
        "\n",
        "Requires extra training\n",
        "\n",
        "4. Optimize the Prompt\n",
        "\n",
        "Idea: Shorter, more direct prompts ‚Üí fewer tokens processed.\n",
        "Why it helps:\n",
        "\n",
        "Less computation per inference\n",
        "\n",
        "Lower latency\n",
        "Trade-off:\n",
        "\n",
        "Model may need fine-tuning to adapt to the shorter format\n",
        "\n",
        "5. Limit max_new_tokens\n",
        "\n",
        "Idea: Reduce output length (e.g., 20‚Äì40 tokens).\n",
        "Why it helps:\n",
        "\n",
        "Direct reduction in compute\n",
        "Trade-off:\n",
        "\n",
        "Answers may become too short"
      ],
      "metadata": {
        "id": "z6xYEIwB_YH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 5: Speed vs. Accuracy Trade-offs\n",
        "How generation parameters affect speed, quality, and consistency\n",
        "\n",
        "Generation parameters like temperature, top-p, top-k, and max_new_tokens directly impact both the speed of inference and the quality/consistency of model outputs. Here is how each parameter creates trade-offs:\n",
        "\n",
        "1. max_new_tokens\n",
        "\n",
        "Lower value ‚Üí faster inference\n",
        "(the model generates fewer tokens)\n",
        "\n",
        "Higher value ‚Üí more complete explanations\n",
        "but slower and sometimes more verbose.\n",
        "\n",
        "Trade-off:\n",
        "Speed ‚ü∑ completeness. Short answers are fast but may miss details.\n",
        "\n",
        "2. Temperature\n",
        "\n",
        "Low temperature (0.1‚Äì0.3)\n",
        "\n",
        "More deterministic\n",
        "\n",
        "Higher consistency\n",
        "\n",
        "Less creative\n",
        "\n",
        "Fewer hallucinations\n",
        "\n",
        "High temperature (0.7+)\n",
        "\n",
        "More diverse answers\n",
        "\n",
        "Higher chance of mistakes or option drift\n",
        "\n",
        "Trade-off:\n",
        "Consistency ‚ü∑ diversity.\n",
        "Lower temperature improves accuracy but removes flexibility.\n",
        "\n",
        "3. Top-p (nucleus sampling)\n",
        "\n",
        "Low top-p (0.5‚Äì0.9)\n",
        "\n",
        "Restricts generation to most probable tokens\n",
        "\n",
        "Improves correctness + stability\n",
        "\n",
        "Slight speed boost\n",
        "\n",
        "High top-p (0.95‚Äì1.0)\n",
        "\n",
        "More creative but less predictable\n",
        "\n",
        "Trade-off:\n",
        "Controlled reasoning ‚ü∑ broader exploration.\n",
        "\n",
        "4. Top-k\n",
        "\n",
        "Low top-k (10‚Äì50)\n",
        "\n",
        "Model considers fewer tokens\n",
        "\n",
        "Faster, more deterministic\n",
        "\n",
        "High top-k (100‚Äì200)\n",
        "\n",
        "More variety but slower + potentially noisier\n",
        "\n",
        "Trade-off:\n",
        "Speed ‚ü∑ linguistic richness."
      ],
      "metadata": {
        "id": "htRbXKOw_jWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part C : Evaluation Methodology"
      ],
      "metadata": {
        "id": "tESshVrr_y2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 7: Improving Evaluation Metrics\n",
        "Limitations of exact / partial match\n",
        "\n",
        "Our current evaluation is very crude:\n",
        "\n",
        "Exact match\n",
        "\n",
        "Checks if the correct letter or exact text appears in the prediction.\n",
        "\n",
        "‚ùó Can miss good answers phrased differently ‚Üí false negatives.\n",
        "\n",
        "Partial match (70% word overlap)\n",
        "\n",
        "Based on token overlap after removing stopwords.\n",
        "\n",
        "‚ùó Counts ‚Äúkeyword soup‚Äù as correct even if reasoning is wrong ‚Üí false positives.\n",
        "\n",
        "‚ùó Fails when the model is right conceptually but uses different wording (synonyms, paraphrases).\n",
        "\n",
        "So yes, we almost certainly have both:\n",
        "\n",
        "False negatives: answer is medically correct, but overlap < 70% or no exact letter mention.\n",
        "\n",
        "False positives: text repeats the right words but chooses the wrong option or wrong conclusion.\n",
        "\n",
        "What could we do better?\n",
        "\n",
        "Score on the choice (A/B/C/D) first\n",
        "\n",
        "Treat the correct letter as the primary signal.\n",
        "\n",
        "Explanation quality can be evaluated separately.\n",
        "\n",
        "‚Üí Reduces false positives where the explanation sounds right but the option is wrong.\n",
        "\n",
        "Use semantic similarity instead of raw word overlap\n",
        "\n",
        "Compare embeddings (e.g., with cosine similarity) between prediction and ground truth.\n",
        "\n",
        "More robust to paraphrasing and synonyms.\n",
        "\n",
        "‚Üí Fewer false negatives.\n",
        "\n",
        "Separate ‚Äúanswer correctness‚Äù and ‚Äúexplanation quality‚Äù\n",
        "\n",
        "Metric 1: did the model choose the correct option? (0/1)\n",
        "\n",
        "Metric 2: is the explanation coherent and medically aligned? (via partial match or human review).\n",
        "\n",
        "‚Üí Gives a more realistic view of model usefulness.\n",
        "\n",
        "Manual review on a small subset\n",
        "\n",
        "For 20‚Äì50 examples, check predictions by hand (or by a medical student).\n",
        "\n",
        "Helps calibrate whether automatic metrics are too harsh or too lenient."
      ],
      "metadata": {
        "id": "hWyA1fAV__o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 8: Test Set Size and Confidence\n",
        "1. Small test sets give unreliable accuracy\n",
        "\n",
        "Accuracy fluctuates a lot when testing on only 5‚Äì20 samples.\n",
        "A few easy or hard questions can completely distort the result.\n",
        "\n",
        "2. Larger test sets give more stable performance\n",
        "\n",
        "When testing on 100+ samples, accuracy becomes more consistent and representative of the model‚Äôs true ability.\n",
        "\n",
        "3. Why performance may drop when test size increases\n",
        "\n",
        "A small sample may be accidentally easy.\n",
        "A larger sample includes harder, more diverse questions ‚Üí accuracy decreases but becomes more realistic.\n",
        "\n",
        "**How to improve evaluation confidence**\n",
        "\n",
        "Use a larger test set (100‚Äì500 samples minimum)\n",
        "\n",
        "Use stratified sampling to cover all medical topics\n",
        "\n",
        "Average results over multiple random seeds"
      ],
      "metadata": {
        "id": "FBENQMG3AUgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part D : Real-World deployment scenario"
      ],
      "metadata": {
        "id": "BSJtLBsEAeiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 9: Production Considerations (Medical Assistance App)\n",
        "1. Safety & Reliability\n",
        "\n",
        "Never use the model alone ‚Üí always behind a human-in-the-loop (doctor/pharmacist validates outputs).\n",
        "\n",
        "Add strong disclaimers: ‚ÄúNot a medical diagnosis. Always consult a physician.‚Äù\n",
        "\n",
        "Block obvious dangerous outputs (e.g. self-medication dosages, stopping treatment) with safety filters and rules.\n",
        "\n",
        "2. Handling Updates\n",
        "\n",
        "Regularly retrain or refresh the model on updated guidelines (e.g. new protocols, drugs withdrawn).\n",
        "\n",
        "Version your models (v1, v2‚Ä¶) and log which version answered which query.\n",
        "\n",
        "Maintain an update process: new data ‚Üí validation ‚Üí staged deployment.\n",
        "\n",
        "3. Edge Cases & Uncertainty\n",
        "\n",
        "Detect low-confidence situations (e.g. conflicting info, rare diseases) and answer:\n",
        "\n",
        "‚ÄúI‚Äôm not confident enough to answer. Please consult a specialist.‚Äù\n",
        "\n",
        "Force the model to refuse out-of-scope questions (legal, financial, etc.).\n",
        "\n",
        "Log all edge-case queries and review them regularly to improve prompts and safeguards.\n",
        "\n",
        "4. Monitoring & Auditing\n",
        "\n",
        "Log queries + responses (with anonymization) to detect harmful patterns.\n",
        "\n",
        "Set up monitoring dashboards (error rates, refusal rates, flagged cases).\n",
        "\n",
        "Allow clinicians to report incorrect or dangerous answers and feed that back into improvement."
      ],
      "metadata": {
        "id": "wRydDfLkAlfd"
      }
    }
  ]
}